---
title: "Homework"
author: "Tianyi Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(comment = NA)
```

# HW0-2022-09-09

## Question

Use knitr to produce at least 3 examples (texts, figures, tables).

## Answer

A dataset of an experiment on the cold tolerance of the grass plants, by measuring the carbon dioxide uptake rate.

```{r}
dataset1 = CO2
head(dataset1)
```

Several plots summarize the data in the dataset CO2 and a regression on it.

```{r, echo=FALSE}
plot(x = dataset1$conc[1:7], y = dataset1$uptake[1:7], type = "l",
     xlim = c(95, 1000), ylim = c(0, 50), xaxs = "i", yaxs = "i",
     main = "Carbon Dioxide Uptake Rates Versus Different Ambient Carbon Dioxide \nConcentrations in Each Enviroment",
     xlab = "ambient carbon dioxide concentrations (mL/L)", ylab = "carbon dioxide uptake rates (umol/m^2/sec)")
for(i in 2:12)
{
  lines(x = dataset1$conc[(7*i-6):(7*i)], y = dataset1$uptake[(7*i-6):(7*i)])
}
```

```{r}
plot(lm(uptake ~ log(conc) + Type + Treatment, data = dataset1))
```

A piece of code which you can use to create a table in a LaTeX file.

```{r}
xtable::xtable(dataset1[c(1:7, 22:28), ])
```


# HW1-2022-09-15

## Question

(Exercise 3.3, Statistical Computating with R)

## Answer

Let $U = F(x)$, it's easy to get the inverse transformation of $F(x)$ $$x = F^{-1}(U) = b(1-U)^{-\frac{1}{a}}, x ≥ b > 0,a > 0.$$
Now we generate 1000 samples from the Pareto(2, 2) distribution by using the inverse transform method.

```{r}
set.seed(114)
U_3 = runif(1000)
sample_3 = 2 * (1-U_3) ^ (-1/2)
```

Finally we graph the density histogram of the sample with the Pareto(2, 2) density.

```{r}
hist(sample_3, prob = TRUE,
     xlim = c(0, 20), breaks = 140,
     main = "Histogram of Samples From Pareto(2,2)", xlab = "x", ylab = "density")
lines(x = seq(from = 2, to = 25, by = 0.1), y = 8 / seq(from = 2, to = 25, by = 0.1) ^ 3)
```

(There are few and scattered samples much bigger than 20. For readability, we don't show them in the plot above.)

## Question

(Exercise 3.7, Statistical Computating with R)

## Answer

The pdf of Beta($a$, $b$) distribution tends to infinity as $x$ tends to 0 while $a$ is no bigger than 1, or as $x$ tends to 1 while $b$ is no bigger than 1. I cannot find a pdf to cover it, so I only discuss the situation where $a$ and $b$ are both bigger than 1.
First, we write the function that generates a random sample of size n from the Beta($a$, $b$) distribution by the acceptance-rejection method. According to the AM-GM inequality, $$x^{a-1}(1-x)^{b-1}\leq [\frac{(a-1)x+(b-1)(1-x)}{a+b-2}]^{\frac{1}{a+b-2}}\leq[\frac{max(a-1, b-1)}{a+b-2}]^{\frac{1}{a+b-2}}, a>1,b>1.$$
Therefore, we set $g \sim U(0,1)$ and $c = Beta(a, b)[\frac{max(a-1, b-1)}{a+b-2}]^{-\frac{1}{a+b-2}}$

```{r}
beta_7 = function(n, a, b)
{
  x = c()
  count1 = 0
  count2 = 0
  while(count2 < n)
  {
    y = runif(1)
    u = runif(1)
    count1 = count1 + 1
    if(u < y ^ (a-1) * (1-y) ^ (b-1) * (max(a-1, b-1) / (a+b-2)) ^ (1 / (a+b-2)))
    {
      x = c(x, y)
      count2 = count2 + 1
    }
  }
  return(list(x, count1, count2))
}
```

Then we use the function to generate 1000 samples from Beta(3, 2)

```{r}
set.seed(514)
sample_7 = beta_7(1000, 3, 2)
sample_7[[2]]
```

This how many iterations have occured in the process above.
Finally we graph the density histogram of the sample with the Beta(3, 2) density.

```{r}
hist(sample_7[[1]], prob = TRUE,
     xlim = c(0, 1), breaks = 50,
     main = "Histogram of Samples From Beta(3,2)", xlab = "x", ylab = "density")
lines(x = seq(from = 0, to = 1, by = 0.01), y = 12 * seq(from = 0, to = 1, by = 0.01) ^ 2 * (1 - seq(from = 0, to = 1, by = 0.01)))
```

## Question

(Exercise 3.12, Statistical Computating with R)

## Answer

```{r}
set.seed(1919)
lambda_12 = rgamma(1000, shape = 4, rate = 2)
sample_12 = rexp(1000, lambda_12)
summary(sample_12)
```

## Question

(Exercise 3.13, Statistical Computating with R)

## Answer

We are informed that the samples in Exercise 3.12 come from the Pareto(4,2) distribution. Now we use the samples to graph the density histogram with the Pareto(4,2) theoretical density. (Noticed the difference between the two formulas in Exercise 3.3 and Exercise 3.13, we should add 2 to each sample before we graph the histogram.)

```{r}
hist(sample_12+2, prob = TRUE,
     xlim = c(0, 9), breaks = 50,
     main = "Histogram of Samples From Pareto(4,2)", xlab = "x", ylab = "density")
lines(x = seq(from = 2, to = 10, by = 0.1), y = 64 / seq(from = 2, to = 10, by = 0.1) ^ 5)
```


# HW2-2022-09-23

## Question

i). For $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1, . . . , n$.

ii). Calculate computation time averaged over $100$ simulations, denoted by $a_n$.

iii). Regress $a_n$ on $t_n := n log(n)$, and graphically show the results (scatter plot and regression line).

## Answer

Firstly, we write a function of the fast sorting algorithm named quicksort.

```{r}
quicksort = function(x, count = 0)
{
  if (length(x) <= 1) return(list(x, count))

  xi = sample(1:length(x), 1)
  flag = x[xi]
  temp = x[-xi]

  indexs = which(temp <= flag)
  count = count + length(indexs)
  x1 = quicksort(temp[indexs], count)
  count = x1[[2]]
  x1 = x1[[1]]

  indexb = which(temp > flag)
  count = count + length(indexb)
  x2 = quicksort(temp[indexb], count)
  count = x2[[2]]
  x2 = x2[[1]]

  return(list(c(x1, flag, x2), count))
}
```

i).

Now we apply our function to samples of randomly permuted numbers of $1, . . . , n, n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$. The codes when $n = 10^4$ are shown below, and the others are concealed.

```{r}
sample1_1 = quicksort(sample(1:10^4))[[1]]
plot(x = 1: 10^4, y = sample1_1, type = "l",
     xaxs = "i", yaxs = "i", xlim = c(0, 10^4), ylim = c(0, 10^4),
     xlab = "1:10000", ylab = "fast-sorted-sample", main = "Fast Sort Randomly Permuted Data of 1 to 10000")
```

```{r, echo = FALSE}
sample1_2 = quicksort(sample(1:(2 * 10^4)))[[1]]
plot(x = 1: (2 * 10^4), y = sample1_2, type = "l",
     xaxs = "i", yaxs = "i", xlim = c(0, 2 * 10^4), ylim = c(0, 2 * 10^4),
     xlab = "1:20000", ylab = "fast-sorted-sample", main = "Fast Sort Randomly Permuted Data of 1 to 20000")
```

```{r, echo = FALSE}
sample1_4 = quicksort(sample(1:(4 * 10^4)))[[1]]
plot(x = 1: (4 * 10^4), y = sample1_4, type = "l",
     xaxs = "i", yaxs = "i", xlim = c(0, 4 * 10^4), ylim = c(0, 4 * 10^4),
     xlab = "1:40000", ylab = "fast-sorted-sample", main = "Fast Sort Randomly Permuted Data of 1 to 40000")
```

```{r, echo = FALSE}
sample1_6 = quicksort(sample(1:(6 * 10^4)))[[1]]
plot(x = 1: (6 * 10^4), y = sample1_6, type = "l",
     xaxs = "i", yaxs = "i", xlim = c(0, 6 * 10^4), ylim = c(0, 6 * 10^4),
     xlab = "1:60000", ylab = "fast-sorted-sample", main = "Fast Sort Randomly Permuted Data of 1 to 60000")
```

```{r, echo = FALSE}
sample1_8 = quicksort(sample(1:(8 * 10^4)))[[1]]
plot(x = 1: (8 * 10^4), y = sample1_8, type = "l",
     xaxs = "i", yaxs = "i", xlim = c(0, 8 * 10^4), ylim = c(0, 8 * 10^4),
     xlab = "1:80000", ylab = "fast-sorted-sample", main = "Fast Sort Randomly Permuted Data of 1 to 80000")
```

ii).

We write another function to calculate average computation time.

```{r}
timing = function(n, times = 100)
{
  t = c()
  for(i in 1:times)
  {
    t = c(t, system.time(quicksort(sample(1:n)))[1])
  }
  return(mean(t))
}
```

Then we apply this function to situations where $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$.

```{r}
set.seed(114514)
a_n = c(timing(10^4), timing(2 * 10^4),timing(4 * 10^4), timing(6 * 10^4), timing(8 * 10^4))
a_n
```

iii).

We regress $a_n$ from question above on $t_n := n log(n)$.

```{r}
t_n = c(10^4 * log(10^4), 2 * 10^4 * log(2 * 10^4), 4 * 10^4 * log(4 * 10^4), 6 * 10^4 * log(6 * 10^4), 8 * 10^4 *log(8 * 10^4))
fit1_3 = lm(a_n ~ t_n)
summary(fit1_3)
```

The t-test on the regression strongly approves that $a_n \sim O(nlog(n))$.

Finally we graph a scatter plot and regression line to reveal the conclusion.

```{r}
plot(x = t_n, y = a_n, pch = 20,
     xlab = expression("t"["n"] * "=O(nlog(n))"), ylab = expression("a"["n"]), main = "Averaged Computation Time in the Fast Sort Algorithm \n on Different Size of Samples")
lines(x = t_n, y = predict(fit1_3))
```

## Question

(Exercise 5.6, Statistical Computating with R)

## Answer

Suppose samples $U_1, U_2, ..., U_{2m} \sim U(0,1)$ are independent identically distributed(i.i.d.), and the simple Monte Carlo estimate of $\hat\theta$ will be $\hat\theta_{MC} := \frac{1}{2m} \sum\limits_{i=1}^{2m} e^{U_i}$. If we use antithetic variables, the estimate will be $\hat\theta_{MCa} := \frac{1}{2m} \sum_\limits{i=1}^{m} (e^{U_i} + e^{1-U_i})$.

Because $$E(e^U) = \int_0^1 e^u du = e-1,$$
and $$E(e^{1-U}) = \int_0^1 e^{1-u} du = e-1,$$
we can compute that $$Cov(e^U, e^{1-U}) = E(e^U e^{1-U})-E(e^U)E(e^{1-U}) = e - (e-1)^2 < 0.$$

Therefore the use of antithetic variate can reduce the variance of $\hat\theta$.

Because $$Var(e^U) = E(e^{2U}) - [E(e^U)]^2 = \int_0^1 e^{2u} du - [E(e^U)]^2 = \frac{1}{2} (e^2-1) - (e-1)^2 = -\frac{1}{2}e^2 + 2e - \frac{3}{2},$$
and $$Var(e^{1-U}) = E(e^{2(1-U)}) - [E(e^{1-U}]^2 = \int_0^1 e^{2(1-u)} du - [E(e^{1-U})]^2 = \frac{1}{2} (e^2-1) - (e-1)^2 = -\frac{1}{2}e^2 + 2e - \frac{3}{2},$$
we can compute that $$Var(e^U+e^{1-U}) = Var(e^U) + Var(e^{1-U}) + 2Cov(e^U, e^{1-U}) = -e^2 + 4e - 3 + 2e - 2(e-1)^2 = -3e^2 + 10e - 5.$$

Thus, $$Var(\hat\theta_{MCa}) = \frac{1}{4m^2}\sum\limits_{i=1}^{m} Var(e^{U_i} + e^{1-U_i}) = \frac{-3e^2 + 10e - 5}{4m},$$
and $$Var(\hat\theta_{MC}) = \frac{1}{4m^2}\sum\limits_{i=1}^{2m} Var(e^{U_i}) = \frac{-\frac{1}{2}e^2 + 2e - \frac{3}{2}}{2m},$$

Above all, the percent reduction in variance of $\hat\theta$ that can be achieved using antithetic variate is $$1 - \frac{Var(\hat\theta_{MCa})}{Var(\hat\theta_{MC})} = 1 - \frac{-3e^2 + 10e - 5}{-e^2 + 4e - 3} = \frac{2e^2 - 6e + 2 }{-e^2 + 4e - 3},$$
which is:

```{r}
(2 * exp(2) - 6 * exp(1) + 2) / (-exp(2) + 4 * exp(1) - 3)
```

## Question

(Exercise 5.7, Statistical Computating with R)

## Answer

According the formulas in Exercise 5.6, we code empirical estimates of the percent reduction in variance using the simple MC and the antithetic variate.(We set $m=1000$ and do $1000$ times Monte Carlo estimates.)

```{r}
set.seed(114514)
theta_MC = c()
theta_MCa = c()
for(i in 1:1000)
{
  sample5_7 = runif(2000)
  theta_MC = c(theta_MC, mean(exp(sample5_7)))
  theta_MCa = c(theta_MCa, mean(c(exp(sample5_7[1:1000]), exp(1 - sample5_7[1:1000]))))
}
```

Here is the density plot of two kinds estimates of $\hat\theta$

```{r}
plot(density(theta_MCa), col = "red",
     xlim = c(1.68, 1.76),
     xlab = "Estimate of θ", main = "Distributions of Estimates Using Two Monte Carlo Methods")
lines(density(theta_MC), col = "blue")
legend("topright", c("Simple MC", "Antithetic Variate"), lty = c(1, 1), col = c("blue", "red"))
abline(v = exp(1) - 1)
```

Following are the estimate of standard deviation of $\hat\theta_{MC}$ using the simple MC method, and the estimate of standard deviation of $\hat\theta_{MCa}$ using the antithetic variate, and the ratio of $\hat{sd}(\hat\theta_{MCa})$ and $\hat{sd}(\hat\theta_{MC})$.

```{r}
c(sd(theta_MC), sd(theta_MCa), sd(theta_MCa) / sd(theta_MC))
```

The empirical estimate of percent reduction in variance using the antithetic variate is:

```{r}
1 - (sd(theta_MCa) / sd(theta_MC)) ^ 2
```

And the theoretical value from Exercise 5.6 is:

```{r}
(2 * exp(2) - 6 * exp(1) + 2) / (-exp(2) + 4 * exp(1) - 3)
```



# HW3-2022-09-30

## Question

(Exercise 5.13, Statistical Computating with R)

## Answer

Firstly, we graph the function image of $g(x)$.

```{r}
x = seq(1, 5, by = 0.01)
plot(x = x, y = x^2 / sqrt(2*pi) * exp(-x^2 / 2), type = "l",
     ylim = c(0, 0.3), ylab = "value of function", main = "g(x)",
     xaxs = "i", yaxs = "i")
```

The form of $g$ seems similar to the pdf of normal distribution, so we choose the pdf of $X_1 := \left| X \right| + 1 , X \sim N(0, 1)$ as $f_1$ in order to 'close' $g$. Now we generate the pdf of $f_1$.

For $\forall x>1$, there is $$ P(X_1 < x) = P(\left| X \right| + 1 < x) = P(1-x < X < x-1) = 1 - 2 \Phi(1-x), $$
where $\Phi$ is the cdf of $N(0, 1)$.

Thus, we generate that $$ f_1 (x) = 2 \Phi ^ \prime (t)|_{t = 1 - x} = \frac{2}{\sqrt{2 \pi}} e^{-\frac{(1-x) ^ 2}{2}}, x > 1$$

Another good idea to 'close' $g$ is let $X_2 := Y + 1, Y \sim \Gamma(2, 2).$ It is easy to generate that $f_2$, the pdf of $X_2$, is $$ f_2(x) = \frac{2^2}{\Gamma(2)} (x-1) e^{-2(x-1)}, x > 1.$$

In an R program, both $X \sim N(0, 1)$ and $Y \sim \Gamma(2, 2)$ can be easily ganerated by the function rnorm() and rgamma(), so $X_1$ and $X_2$ are both easy to approach.

Now we add the two function images of $f_1$ and $f_2$ to the graph above. (Because $\int^{\infty}_1 g dx \approx 0.4$ and $\int^{\infty}_1 f_1 dx = \int^{\infty}_1 f_2 dx = 1$, we use the image of $2.5g$ instead of the image of $g$.)

```{r}
plot(x = x, y = 2.5 * x^2 / sqrt(2*pi) * exp(-x^2 / 2), type = "l",
     ylim = c(0, 1), ylab = "value of function", main = "(2.5)g(x) and It's 'Close'",
     xaxs = "i", yaxs = "i")
lines(x = x, y = 2 / sqrt(2*pi) * exp(-(1-x)^2 / 2), col = "red")
lines(x = x, y = 4 / gamma(2) * (x-1) * exp(-2 * (x-1)), col = "blue") #or dgamma(x-1, 2, 2)
legend("topright", c("2.5g(x)", expression("f"[1]*"(x)"), expression("f"[2]*"(x)")), col = c("black", "red", "blue"), lty = rep(1, 3))
```

Next we use the importance sampling method to estimate $$\theta = \int^{\infty}_{1} \frac{x^2}{\sqrt{2\pi}} e^{−x^2/2} dx.$$

```{r}
g13 = function(x)
{
  return(x^2 / sqrt(2*pi) * exp(-x^2 / 2))
}

f113 = function(x)
{
  return(2 / sqrt(2*pi) * exp(-(1-x)^2 / 2))
}

f213 = function(x)
{
  return(4 / gamma(2) * (x-1) * exp(-2 * (x-1)))
}
```

```{r}
set.seed(114514)
m13 = 100000
sample13_1 = abs(rnorm(m13)) + 1
grf13_1 = numeric(m13)
for(i in 1:m13)
{
  grf13_1[i] = g13(sample13_1[i]) / f113(sample13_1[i])
}

sample13_2 = rgamma(m13, 2, 2) + 1
grf13_2 = numeric(m13)
for(i in 1:m13)
{
  grf13_2[i] = g13(sample13_2[i]) / f213(sample13_2[i])
}
```

```{r}
c(mean(grf13_1), mean(grf13_2))
```

Contrasted with the theoretical value:

```{r}
integrate(g13, lower = 1, upper = "infinity")
```

The variance of $\hat\theta$ (${Var} (\hat\theta)$) of the improtance function $f_1$ and $f_2$ are seperately:

```{r}
c(var(grf13_1), var(grf13_2))
```

We can infer that the FIRST importance function produce the smaller variance.

Let us back to the theoritical variance of $\hat\theta$ ($Var (\hat\theta)$), which is $$ \frac{1}{m} Var(\frac{g(X)}{f(X)}), $$ where $X \sim f$.

Because we use the same $m$ in both $f_1$ and $f_2$, and $$E(\frac{g(X)}{f(X)}) = \int^{\infty}_1 \frac{g(x)}{f(x)} \cdot f(x) dx = \int^{\infty}_1 g(x) dx,$$

which implies $E(g(X) / f(X))$ does not depend on the choose of $f$, the problem comes into the comparasion between $$E(\frac{g^2 (x)}{f^2_i (x)}), i = 1, 2.$$

Because $$E(\frac{g^2 (x)}{f^2_1 (x)}) = \int^{\infty}_1 \frac{g^2 (x)}{f_1 (x)} dx = \int^{\infty}_1 \frac{x^4}{4\sqrt{2\pi}} e^{−x^2 + \frac{(1-x)^2}{2}} dx,$$

and $$E(\frac{g^2 (x)}{f^2_2 (x)}) = \int^{\infty}_1 \frac{g^2 (x)}{f_2 (x)} dx = \int^{\infty}_1 \frac{x^4}{2\pi \cdot 4(x-1)} e^{−x^2 + 2(x-1)} dx,$$

with the help of computer,

```{r}
c(integrate(function(x) {return(x^4 / 4 / sqrt(2 * pi) * exp(-x^2 + (1-x) ^ 2 / 2))}, lower = 1, upper = "infinite")$value,
  integrate(function(x) {return(x^4 / (2 * pi) / 4 / (x-1) * exp(-x^2 + 2 * (x-1)))}, lower = 1.01, upper = "infinite")$value)
```

we finally get $$E(\frac{g^2 (x)}{f^2_1 (x)}) = \int^{\infty}_1 \frac{x^4}{4\sqrt{2\pi}} e^{−x^2 + \frac{(1-x)^2}{2}} dx < \int^{\infty}_{1.01} \frac{x^4}{2\pi \cdot 4(x-1)} e^{−x^2 + 2(x-1)} dx < \int^{\infty}_1 \frac{x^4}{2\pi \cdot 4(x-1)} e^{−x^2 + 2(x-1)} dx = E(\frac{g^2 (x)}{f^2_2 (x)}).$$

This is consistant with our conclusion that the FIRST importance function produce the smaller variance from the empirical simulation above.

## Question

(Exercise 5.15, Statistical Computating with R)

## Answer

In Example 5.13, the function to be integrated is $g(x) = e ^ {-x} / (1 + x^2), 0 < x < 1$ and the importance function is $f(x) = e ^ {-x} / (1 - e ^ {-1}), 0 < x < 1$. Then the stratified importance sampleing method divides the interval $(0, 1)$ into five subintervals, which are $(j/5,(j + 1)/5), j = 0, 1,..., 4.$

Thus on the $j^{th}$ subinterval variables $X_j$ are generated from the density owning the form like $f(x) = e ^ {-x} / (1 - e ^ {-1}), j/5 < x < (j+1) / 5$.

Because $$c_j := \int^{\frac{j+1}{5}}_{\frac{j}{5}} \frac{e ^ {-x}}{1 - e ^ {-1}} dx = \frac{e^{-\frac{j}{5}} (1 - e^{-\frac{1}{5}})}{1 - e^{-1}}, $$

we can gain the pdf subinterval variables generated from $$ X_j \sim f_j (x) := \frac{f(x)}{c_j} = \frac{e^{-x}}{e^{-\frac{j}{5}} (1 - e^{-\frac{1}{5}})} , \frac{j}{5} < x < \frac{j+1}{5},$$

Therefore $$ F_j (x) = \frac{e^{-\frac{j}{5}} - e^{-x}}{e^{-\frac{j}{5}} (1 - e^{-\frac{1}{5}})}, \frac{j}{5} < x < \frac{j+1}{5}, $$

and $$ F^{-1}_j (u) = -log(e^{-\frac{j}{5}} - ue^{-\frac{j}{5}} (1 - e^{-\frac{1}{5}})), 0 < u < 1 $$

Now we use the stratified importance sampling method with a division mentioned above to estimate $$ \theta = \int^1_0  e ^ {-x} / (1 + x^2) dx.$$

```{r}
F_inverse15 = function(u, j)
{
  return(-log(exp(-j / 5) - u * exp(-j / 5) * (1 - exp(-1 / 5))))
}

g15 = function(x)
{
  return(exp(-x) / (1 + x ^ 2))
}

f15 = function(x, j)
{
  return(exp(-x) / exp(-j / 5) / (1 - exp(-1 / 5)))
}
```

```{r}
set.seed(114514)
est15 = numeric(50)

for(i in 1:50)
{
  U15 = runif(10000)

  sample15_1 = numeric(2000)
  for(j in 1:2000)
  {
    X = F_inverse15(U15[j], 0)
    sample15_1[j] = g15(X) / f15(X, 0)
  }

  sample15_2 = numeric(2000)
  for(j in 1:2000)
  {
    X = F_inverse15(U15[j + 2000], 1)
    sample15_2[j] = g15(X) / f15(X, 1)
  }

  sample15_3 = numeric(2000)
  for(j in 1:2000)
  {
    X = F_inverse15(U15[j + 4000], 2)
    sample15_3[j] = g15(X) / f15(X, 2)
  }

  sample15_4 = numeric(2000)
  for(j in 1:2000)
  {
    X = F_inverse15(U15[j + 6000], 3)
    sample15_4[j] = g15(X) / f15(X, 3)
  }

  sample15_5 = numeric(2000)
  for(j in 1:2000)
  {
    X = F_inverse15(U15[j + 8000], 4)
    sample15_5[j] = g15(X) / f15(X, 4)
  }

  est15[i] = sum(c(mean(sample15_1), mean(sample15_2), mean(sample15_3), mean(sample15_4), mean(sample15_5)))
}
```

```{r}
mean(est15)
```

This is the value of $\hat\theta^{SI}$, and $\hat\theta^I$ use the importance sampling method in Exercise 5.13 is $0.5257801$.

Finally we compare the variance between $\hat\theta^{SI}$ and $\hat\theta^I$, which are successively given below:

```{r}
var(est15)
```

and

```{r}
0.0970314 ^ 2 # This data comes from Exercise 5.13.
```

It is obiously that $Var(\hat\theta^{SI}) < Var(\hat\theta^I)$.



# HW4-2022-10-09

## Question

(Exercise 6.4, Statistical Computating with R)

## Answer

I will answer this and following questions in form of that required by the pdf as possible. Some illustration may turn to be in form of annotation in the R code chunks.

```{r}
sample4 = function(n, meanlog, sdlog) # data generation
{
  return(rlnorm(n, meanlog, sdlog))
}
```

```{r}
#test sample4()
set.seed(114514)
hist(sample4(1000, 0, 0.5), breaks = 25, freq = FALSE)
lines(x = seq(0, 5, 0.01), y = dlnorm(seq(0, 5, 0.01), sdlog = 0.5))
```

Because $X \sim logN(\mu, \sigma^2)$ equals $log X \sim N(\mu, \sigma^2)$, we can simply transfer this question into the circunstance of normal distribution by taking logarithm of the samples.

```{r}
CI4 = function(m = 1000, n = 20, meanlog = 0, sdlog = 1, alpha = 0.05) # data analysis
{
  mu_hat = numeric(m)
  sd_hat = numeric(m)
  UCL = numeric(m)
  DCL = numeric(m)
  y = numeric(m)
  for(i in 1:m)
  {
    data = sample4(n, meanlog, sdlog)
    mu_hat[i] = mean(log(data))
    sd_hat[i] = sd(log(data)) / sqrt(n)
    UCL[i] = mu_hat[i] + qt(1 - alpha / 2, n - 2) * sd_hat[i]
    DCL[i] = mu_hat[i] + qt(alpha / 2, n - 2) * sd_hat[i]
    y[i] = 1 - (meanlog < DCL[i]) - (meanlog > UCL[i])
  }
  return(list(mu = mu_hat, sd = sd_hat, UCL = UCL, DCL = DCL, CP = mean(y)))
}
```

```{r}
#test CI4, we show a single estimate confidence interval. Following data is the estimate mean, standard deviation, DCL, UCL of the parameter.
set.seed(114514)
temp = CI4(m = 1)
c(temp$mu, temp$sd, temp$DCL, temp$UCL)
rm(temp)
```

```{r}
resulting4 = function(m = 1000, n = 20, meanlog = 0, sdlog = 1, alpha = 0.05) # result reporting (these parameters seem a bit confusing)
{
  temp = CI4(m, n, meanlog, sdlog, alpha)

  #plot, where we show some simulative CIs as examples.
  m1 = min(m, 20)
  plot(x = 1:m1, y = temp$UCL[1:m1], type = "l",
       ylim = c(min(temp$DCL[1:m1]), max(temp$UCL[1:m1])),
       xlab = paste("μ = ", meanlog, ", σ = ", sdlog), ylab = "CI of μ",
       main = "Some Simulative Confidence Intervals of a Lognormal Distribution")
  lines(x =1:20, y = temp$DCL[1:m1])
  abline(h = meanlog, col = "grey")

  #estimate CP
  return(temp$CP)
}
```

An example where $m = 10000, n = 20, \mu = 0, \sigma = 1, \alpha = 0.05$:

```{r}
set.seed(114514)
print(paste("The estimated CP is", resulting4(m = 10000)))
```

Another example where $m = 1000, n = 50, \mu = 2, \sigma = 10, \alpha = 0.01$:

```{r}
set.seed(114514)
print(paste("The estimated CP is", resulting4(n = 50, meanlog = 2, sdlog = 10, alpha = 0.01)))
```

Both estimated CP are close to $1 - \alpha$.

## Question

(Exercise 6.8, Statistical Computating with R)

## Answer

The following function is based on the code of the example of Count Five tset on the book.

And because the samples just come from normal distribution, there is no need to write another data generater function. Thus we straightly start from the data analysis function,

```{r}
count5test8 = function(x, y) # data analysis
{
  X = x - mean(x)
  Y = y - mean(y)
  outx = sum(X > max(Y)) + sum(X < min(Y))
  outy = sum(Y > max(X)) + sum(Y < min(X)) # return 1 (reject H0) or 0 (do not reject H0)
  reject = as.integer(max(c(outx, outy)) > 5)
  return(reject)
}

Ftest8 = function(x, y, alpha = 0.95)
{
  reject = as.integer(var.test(x, y, alternative = "two.sided", conf.level = 1 - alpha)[[3]] < alpha)
  return(reject)
}
```

```{r}
#test functions above, through a single hypothesis test.
set.seed(114514)
x = rlnorm(1000, 0, 1)
y = rlnorm(1000, 0, 1.2)
c(count5test8(x, y), Ftest8(x, y))
rm(x)
rm(y)
```

```{r}
resulting8 = function(m = 1000, n = 20, sigma1 = sigma1, sigma2 = sigma2, alpha = 0.055) # result reporting
{
  power_c5 = numeric(length(n))
  power_F = numeric(length(n))
  for(j in 1:length(n))
  {
    reject1 = numeric(m)
    reject2 = numeric(m)
    for(i in 1:m)
    {
      x = rnorm(n[j], 0, sigma1)
      y = rnorm(n[j], 0, sigma2)
      reject1[i] = count5test8(x, y)
      reject2[i] = Ftest8(x, y, alpha)
    }
    power_c5[j] = mean(reject1)
    power_F[j] = mean(reject2)
    rm(reject1)
    rm(reject2)
  }
  return(cbind(n, power_c5, power_F))
}
```

We let $n = 20, 100, 500, 2000$ (small, medium, medium, big) seperately.

```{r}
set.seed(114514)
resulting8(n = c(20, 100, 500, 2000), sigma1 = 1, sigma2 = 1.5)
```

We can see the $F$ test has MORE power than the Count Five test. The reason is the sample comes from normal distribution, which is an assumption of $F$ test.

## Question

Discussion:

If we obtain the powers for two methods under a particular simulation setting with $10,000$ experiments: say, $0.651$ for one method and $0.676$ for another method. Can we say the powers are different at $0.05$ level?

What is the corresponding hypothesis test problem?

Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

Please provide the least necessary information for hypothesis testing.

## Answer

#### i)
Corresponding hypothesis test is testing whether two researching targets are the same by testing their parameters are the same or not, with an assumption of a consistent form of both targets. (This is a parameter method.)

In this question, the power is the probability to examine the type-II error, so we can set a hyperthesis testing problem as following:

$n = 10000$, samples $X_1, ... , X_n i.i.d. \sim b(p_1)$ and $Y_1, ... , Y_n i.i.d. \sim b(p_2)$. However, $X_i$ and $Y_i$ may be not independent, because they come from testing the same sample. Given $\hat p_1 = \sum ^n _{i=1} X_i / n = 0.651$ and $\hat p_2 = \sum ^n _{i=1} Y_i / n = 0.676$, test $$ H_0 : p_1 - p_2 = 0 \longleftrightarrow H_a : p_1 - p_2 \neq 0.$$

#### ii)
According to analysis above, the Z-test, two sample t-test and paired t-test are not the best, especially when $n$ is not big enough, because samples are not from normal distributions. Thus we consider to use the McNemar test.

#### iii)
The McNemar test statistic is $$ \chi ^2 = \frac {(n_{10} - n_{01}) ^ 2} {n_{10} + n_{01}} \sim \chi ^2 _1, $$

where $n_{ab}$ stands for the situation $a$ and $b$ happen at the same time. ($a=0$ means the count the first test correctly rejects $H^\star_0$ and $a=1$ means the count the first test wrongly accepts $H^\star_0$, and $b$ is similar).

However, we only know $n \hat p_1 = 6510 = n_{00} + n_{01}$ and $n \hat p_2 = 6760 = n_{00} + n_{10}$. We can not gain $n_{10} + n_{01}$ through these data (but can gain this during calculating the powers). So without additional information such as $n_{00}$ or $\hat p_{00}$, we can not judge whether the powers are different or not.


# HW5-2022-10-14

## Question

(Exercise 7.4, Statistical Computating with R)

## Answer

Assume $X_1,X_2,...,X_n i.i.d. \sim Exp(\lambda)$. A sample from an exponentional distribution has the pdf $$ f(x,\lambda) = \lambda e^{- \lambda x} $$.

The likelihood function is $$ L(\lambda;x_1, ... ,x_n) = \lambda ^ n e ^ {-n \overline x \lambda}. $$

Then $$ \frac{\partial ln(L)}{\partial{\lambda}} = \frac{n}{\lambda} - n \overline x. $$

Let $$ \frac{\partial ln(L)}{\partial{\lambda}} = 0, $$

and we can get the MLE of $\lambda$ is $$ \hat\lambda = \frac{1}{\overline X}. $$

Next we use the bootstrap method to estimate $\lambda$.

```{r}
set.seed(114514)
sample4 = c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
B4 = 10000
lambdastar4 = numeric(B4)
lambda4 = 1 / mean(sample4)
for(i in 1:B4)
{
  lambdastar4[i] = 1 / mean(sample(sample4, replace=TRUE))
}
hist(lambdastar4, breaks = 20, freq = FALSE)
abline(v = lambda4, col = "red", lwd = 2)
print(c(lambda.bootest = mean(lambdastar4), lambda.MLE = lambda4,
        bias = mean(lambdastar4) - lambda4,
        se.boot = sd(lambdastar4)))
```

```{r}
rm(lambdastar4)
```

## Question

(Exercise 7.5, Statistical Computating with R)

## Answer

```{r}
set.seed(114514)
boot.mean = function(dat, index) {return(mean(dat[index]))}
sampleb5 = boot::boot(data = sample4, statistic = boot.mean, R = 2000)
ci5 = boot::boot.ci(boot.out = sampleb5, type = c("norm", "basic", "perc", "bca"))
ci.norm5 = ci5$norm[2:3]
ci.basic5 = ci5$basic[4:5]
ci.perc5 = ci5$perc[4:5]
ci.BCa5 = ci5$bca[4:5]
ans5 = rbind(ci.norm5, ci.basic5, ci.perc5, ci.BCa5)
rownames(ans5) = c("norm", "basic", "percentile", "BCa")
colnames(ans5) = c("confidence.l", "confidence.h")
print(ans5)
```

Because samples does not come from a normal distribution, the position of these confidence intervals on parameter space may be not the same. Also, the pdf of $\overline X$ is not symmetric, which may lead differcence among the position of the confidence intervals, too. (For example, the confidence interval of standard bootstrap is on the left of the confidence interval of percentile bootstrap in this question.)

```{r}
rm(ci5)
rm(ci.norm5)
rm(ci.basic5)
rm(ci.perc5)
rm(ci.BCa5)
rm(ans5)
```

## Question

(Project 7.A, Statistical Computating with R)

## Answer

```{r}
boot.mean = function(dat, index)
{
  return(mean(dat[index]))
}
```

We write a function with the help of the R package boot to gain the empirical bootstrap confidence intervals of the parameters and calculate the data we want.

```{r}
bootciA = function(mu, sigma, n = 10, m = 1000)
{
  ci.norm = matrix(NA, m, 2)
  ci.basic = matrix(NA, m, 2)
  ci.perc = matrix(NA, m, 2)

  ci.norm.cover = numeric(m)
  ci.basic.cover = numeric(m)
  ci.perc.cover = numeric(m)

  ci.norm.ml = numeric(m)
  ci.basic.ml = numeric(m)
  ci.perc.ml = numeric(m)

  ci.norm.mr = numeric(m)
  ci.basic.mr = numeric(m)
  ci.perc.mr = numeric(m)

  for(i in 1:m)
  {
    sam = rnorm(n, mu, sigma)
    samb = boot::boot(data = sam, statistic = boot.mean, R = 2000)
    ci = boot::boot.ci(boot.out = samb, type = c("norm", "basic", "perc"))
    ci.norm[i, ] = ci$norm[2:3]
    ci.basic[i, ] = ci$basic[4:5]
    ci.perc[i, ] = ci$percent[4:5]

    mu.sam = mean(sam)
    ci.norm.ml[i] = ci.norm[i, 2] < mu.sam
    ci.norm.mr[i] = ci.norm[i, 1] > mu.sam
    ci.norm.cover[i] = 1 - ci.norm.ml[i] - ci.norm.mr[i]

    ci.basic.ml[i] = ci.basic[i, 2] < mu.sam
    ci.basic.mr[i] = ci.basic[i, 1] > mu.sam
    ci.basic.cover[i] = 1 - ci.basic.ml[i] - ci.basic.mr[i]

    ci.perc.ml[i] = ci.perc[i, 2] < mu.sam
    ci.perc.mr[i] = ci.perc[i, 1] > mu.sam
    ci.perc.cover[i] = 1 - ci.perc.ml[i] - ci.perc.mr[i]

  }
  return(rbind(c(mean(ci.norm.cover), mean(ci.norm.ml), mean(ci.norm.mr)),
               c(mean(ci.basic.cover), mean(ci.basic.ml), mean(ci.basic.mr)),
               c(mean(ci.perc.cover), mean(ci.perc.ml), mean(ci.perc.mr))))
}
```

The answer is shown below.

```{r}
set.seed(114514)
ansA = bootciA(mu = 2, sigma = 2)
```

```{r}
colnames(ansA) = c("coveraged.p", "missed.l", "missed.r")
rownames(ansA) = c("norm", "basic", "perc")
ansA
```

(This question seems confused, because it asks us to check the empirical coverage rates for the SAMPLE mean, not $\mu$, which means the bootstrap confidence interval based on a sample must cover the mean of the sample. This leads the coverage rate shold be $1$, and the missed porpotion should be $0$. This is meaningless. I have tried using the confidences interval to contrast with the origin parameter $mu$ instead of sample mean and gained a result which can explain the confidence coefficient.)

```{r}
rm(ansA)
```



# HW6-2022-10-21

(Exercise 7.8, Statistical Computating with R)

## Answer

```{r}
principal.jack = function(dat8) # Calculate the estimate propotion of the first principal component of a matrix using jacknife
{
  n8 = dim(dat8)[1]
  p8 = dim(dat8)[2]
  lambda8 = eigen(cov(dat8))$values
  theta8 = lambda8[1] / sum(lambda8) # normal estimate value

  lambda.jack8 = matrix(nrow = n8, ncol = p8)
  theta.jack8 = numeric(n8)
  for(i in 1:n8)
  {
    lambda.jack8[i, ] = eigen(cov(dat8[-i, ]))$values
    theta.jack8[i] = lambda.jack8[i, 1] / sum(lambda.jack8[i, ])
  }

  return(c("theta.hat" = theta8,
           "theta.jack.mean" = mean(theta.jack8),
           "bias.jack" = (n8 - 1) * (mean(theta.jack8) - theta8),
           "sd.jack" = sqrt((n8 - 1) / n8 * sum((theta.jack8 - mean(theta.jack8)) ^ 2))))
}
```

```{r}
dat8 = bootstrap::scor
principal.jack(dat8)
```

The values above are $\hat\theta, \overline {\hat \theta}_{(\cdot)}, \widehat{E(\hat\theta) - \theta}, \hat{SE}(\hat\theta)$ seperately.

```{r}
rm(dat8)
```

## Question

(Exercise 7.11, Statistical Computating with R)

## Answer

```{r}
leaveto = function(dat11) # Using the leave-two-out CV to calculate the MSE of each model above.
{
  n11 = dim(dat11)[1]

  eps11_1 = matrix(nrow = n11 * (n11 - 1) / 2, ncol = 2)
  count = 0
  for(i in 1:(n11-1))
  {
    for(j in (i+1):n11)
    {
      count = count + 1
      fit = lm(y ~ x, data = data.frame(y = dat11$magnetic[-c(i, j)],
                                        x = dat11$chemical[-c(i, j)]))
      eps11_1[count, ] = dat11$magnetic[c(i, j)] - predict(fit,
                                                           newdata = data.frame(x = dat11$chemical[c(i, j)]))
    }
  }
  sigma2.hat11_1 = mean(eps11_1^2)

  eps11_2 = matrix(nrow = n11 * (n11 - 1) / 2, ncol = 2)
  count = 0
  for(i in 1:(n11-1))
  {
    for(j in (i+1):n11)
    {
      count = count + 1
      fit = lm(y ~ x + x2,
               data = data.frame(y = dat11$magnetic[-c(i, j)],
                                 x = dat11$chemical[-c(i, j)],
                                 x2 =  dat11$chemical[-c(i, j)] ^ 2))
      eps11_2[count, ] = dat11$magnetic[c(i, j)] - predict(fit,
                                                           newdata = data.frame(x = dat11$chemical[c(i, j)],
                                                                                x2 = dat11$chemical[c(i, j)] ^ 2))
    }
  }
  sigma2.hat11_2 = mean(eps11_2^2)

  eps11_3 = matrix(nrow = n11 * (n11 - 1) / 2, ncol = 2)
  count = 0
  for(i in 1:(n11-1))
  {
    for(j in (i+1):n11)
    {
      count = count + 1
      fit = lm(y ~ x, data = data.frame(y = log(dat11$magnetic[-c(i, j)]),
                                        x = dat11$chemical[-c(i, j)]))
      eps11_3[count, ] = dat11$magnetic[c(i, j)] - exp(predict(fit,
                                                               newdata = data.frame(x = dat11$chemical[c(i, j)])))
    }
  }
  sigma2.hat11_3 = mean(eps11_3^2)

  eps11_4 = matrix(nrow = n11 * (n11 - 1) / 2, ncol = 2)
  count = 0
  for(i in 1:(n11-1))
  {
    for(j in (i+1):n11)
    {
      count = count + 1
      fit = lm(y ~ x, data = data.frame(y = log(dat11$magnetic[-c(i, j)]),
                                        x = log(dat11$chemical[-c(i, j)])))
      eps11_4[count, ] = dat11$magnetic[c(i, j)] - exp(predict(fit,
                                                               newdata = data.frame(x = log(dat11$chemical[c(i, j)]))))
    }
  }
  sigma2.hat11_4 = mean(eps11_4^2)

  return(matrix(c(sigma2.hat11_1, sigma2.hat11_2, sigma2.hat11_3, sigma2.hat11_4), nrow = 1,
                dimnames = list(c("sigma2.hat"), c("Model I", "Model II", "Model III", "Model IV"))))
}
```

```{r}
dat11 = DAAG::ironslag
leaveto(dat11)
```

We can see the Model II has the smallest prediction error through the leave-two-out CV, which is the best. The result is consistant with the best model chosen in Example 7.18 where the leave-one-out CV is used.

```{r}
rm(dat11)
```

## Question

(Exercise 8.2, Statistical Computating with R)

## Answer

In this question, $$ H_0: \rho = 0 \longleftrightarrow H_1: \rho \neq 0.$$

If we consider accept $H_0$, the sample would be uncorrelated. Thus the permutation of the sample should be uncorrelated, too.

The p-value of permutation test is $$ P(\left| \rho^* \right| > \left| \rho \right|),$$

where $\rho$ is the spearman correlation coefficient of original sample and $\rho^*$ is the spearman correlation coefficient of permutation sample.

```{r}
spearman.per = function(x, y, m = 10000)
{
  rho = cor(x, y, method = "spearman")
  p.value = cor.test(x, y, method = "spearman")$p.value
  n1 = length(x)
  n2 = length(y)

  z = c(x, y)
  rho.per = numeric(m)
  for(i in 1:m)
  {
    z.per = sample(z)
    rho.per[i] = cor(z.per[1:n1], z.per[(n1+1):(n1+n2)], method = "spearman")
  }

  p.value.per = sum(abs(rho) < abs(rho.per)) / m
  return(c(rho = rho, p.value = p.value, p.value.per = p.value.per))
}
```

We generate samples in following method:

$$X_i = i, i = 1, ... , 10$$

and

$$Y_i = X_i + \epsilon_i, \epsilon_i i.i.d. \sim N(0, 9) $$

```{r}
set.seed(114514)
x = 1:10
y = x + rnorm(10, 0, 3)
rbind(x, y)
```

```{r}
set.seed(114514)
spearman.per(x, y)
```

We can see the p-values from the function cor.test and the permutation test are similar and they both suggest rejecting $H_0$ when the confidence is $0.10$.

```{r}
rm(x)
rm(y)
```



# HW7-2022-10-28

(Exercise 9.4, Statistical Computating with R)

## Answer

Target density is $$ f(x) = \frac{1}{2} e ^ {-\left| x \right|}, x \in \mathbb{R} $$

The random walk Metropolis proposal distribution satisfies that $$ g(r|s) = \frac{1}{\sqrt{2\pi \sigma^2}} e ^ {-\frac{(r - s) ^ 2}{2 \sigma^2}} = g(s|r)$$

Thus the acceptance probability is $$ \alpha(s, r) = min\{ \frac{f(r)}{f(s)}, 1 \}  = min\{ e^{\left| s \right| - \left| r \right|}, 1 \} $$

```{r}
RWMetroplois = function(ini, sigma, m = 10000)
{
  x = numeric(m)
  x[1] = ini
  count = 0
  for(t in 2:m)
  {
    y = rnorm(1, x[t - 1], sigma) #generate
    u = runif(1)
    if(u <= min(exp(abs(x[t - 1]) - abs(y)), 1))
    {
      x[t] = y
    }
    else
    {
      x[t] = x[t - 1]
      count = count + 1
    }
  }
  return(list(x = x, rejectrate = count / m))
}
```

Now we generate 4 sample chains with initial value $x_0 = 5$ and $\sigma = 0.05, 0.25, 1, 5$ and $m = 10000$.

```{r}
set.seed(114514)
x0 = 5
x41 = RWMetroplois(x0, 0.05)
x42 = RWMetroplois(x0, 0.25)
x43 = RWMetroplois(x0, 1)
x44 = RWMetroplois(x0, 5)
```

The reject rates:

```{r}
rej4 = cbind(c(0.05, 0.25, 1, 5), c(x41$rejectrate, x42$rejectrate, x43$rejectrate, x44$rejectrate))
colnames(rej4) = c("sigma", "reject.rate")
rej4
```

The MC chains:

```{r, eval=FALSE}
par(mfrow = c(2, 2))
plot(x41$x, type = "l", xlab = "sigma = 0.05", ylab = "Xt")
plot(x42$x, type = "l", xlab = "sigma = 0.25", ylab = "Xt")
plot(x43$x, type = "l", xlab = "sigma = 1", ylab = "Xt")
plot(x44$x, type = "l", xlab = "sigma = 5", ylab = "Xt")
```

We can see the chain generated with $\sigma = 0.05$ does not converged in the first $10000$ steps, but other three chains converge in $10000$ steps.

In addition, we use the Gelman-Rubin method to monitor convergence of the chains. We write a function to compute the Gelman-Rubin statistic $= \sqrt{\hat R}$ of a group of sample chains first.

```{r}
GelmanRubin = function(phi)
{
  k = nrow(phi)
  n = ncol(phi)

  phi.chain.mean = rowMeans(phi)
  Bn = n * var(phi.chain.mean)

  phi.chain.variance = apply(phi, 1, "var")
  Wn = mean((n - 1) / n * phi.chain.variance)

  phi.variance.hat = (n - 1) / n * Wn + 1 / n * Bn

  GR = sqrt(phi.variance.hat / Wn)
  return(GR)
}
```

Then we apply this function to this question to check the convergence of chains generated with $\sigma = 0.05, 0.25, 1, 5$. The initial value comes from $N(0, 5)$. And noticed that the situation where $\sigma = 0.05$ does not converge in $10000$ steps, we will straightly compute the Gelman-Rubin statistic to support the conclusion.

```{r}
set.seed(114514)
k = 20
m = 10000
sigma = c(0.05, 0.25, 1, 5)
n.converge = numeric(3)
for(i in 1:4)
{
  ini = rnorm(k, 0, 5)
  phi = data.frame()
  for(j in 1:k)
  {
    phi = rbind(phi, RWMetroplois(ini[j], sigma[i], m)$x)
  }
  if(i == 1)
  {
    GR1 = GelmanRubin(as.matrix(phi))
  }
  else
  {
    for(n in 2:m)
    {
      GR = GelmanRubin(as.matrix(phi)[, 1:n])
      if(GR < 1.2)
      {
        n.converge[i-1] = n
        break
      }
    }
  }
}
```

The Gelman-Rubin statistic when $\sigma = 0.05$:

```{r}
GR1
```

It is still far away from $1.2$, which can support our conclusion.

The estimate steps chains when $\sigma = 0.25, 1, 5$ needs to converge:

```{r}
as.matrix(cbind(sigma = c(0.25, 1, 5), step.converge = n.converge))
```

```{r}
rm(phi)
rm(x41)
rm(x42)
rm(x43)
rm(x44)
```

## Question

(Exercise 9.7, Statistical Computating with R)

## Answer

Target distribution is $$ (X, Y) \sim N_2 (0, \Sigma), \Sigma = \left( \begin{array}{l} 1 & 0.9 \\ 0.9 & 1 \end{array} \right). $$

Thus $$ X|Y \sim N(0.9Y, 0.19), $$

and $$ Y|X \sim N(0.9X, 0.19). $$

```{r}
Gibbs = function(ini, mu1, mu2, sigma1, sigma2, rho, n = 10000, burn = 1000)
{
  x = matrix(0, n, 2)
  s1 = sqrt(1 - rho ^ 2) * sigma1
  s2 = sqrt(1 - rho ^ 2) * sigma2

  x[1, ] = ini
  for (i in 2:n)
  {
    x2 = x[i - 1, 2]
    m1 = mu1 + rho * (x2 - mu2) * sigma1/sigma2
    x[i, 1] = rnorm(1, m1, s1)
    x1 = x[i, 1]
    m2 = mu2 + rho * (x1 - mu1) * sigma2/sigma1
    x[i, 2] = rnorm(1, m2, s2)
  }

  return(x[(burn + 1):n, ])
}
```

Now we generate the bivaariate normal chain by a Gibbs sampler. The plot is a burn-in sample where the first sample points are removed.

```{r}
set.seed(114514)
x7 = Gibbs(ini = c(5, 5), 0, 0, 1, 1, 0.9)
plot(x7, cex = .5, xlab = "X", ylab = "Y", pch = 20)
```

```{r}
fit7 = lm(y ~ x, data = data.frame(y = x7[, 2], x = x7[, 1]))
summary(fit7)
```

The F-test of the regression support the constant variance of the model.

We graph a QQplot to check the normality of residuals:

```{r}
qqplot = function(fit)
{
  eps = fit$residuals
  n = length(eps)
  q = qnorm(seq(0, 1, by = 1 / n)[-1] - 0.5 / n)
  plot(x = q, y = sort(eps), pch = 20, xlab = "qnorm", ylab = "residuals")
}
qqplot(fit7)
```

This QQplot suggest the normality of residuals.

Now we usethe Gelman-Rubin method to check the convergence of chains and whether setting the burn-in $= 1000$ is suitable.

```{r}
set.seed(114514)
k = 20
m = 10000
ini = matrix(c(rnorm(k, -5, 5), rnorm(k, 5, 5)), ncol = 2)
phix = data.frame()
phiy = data.frame()
for(j in 1:k)
{
  temp = Gibbs(ini[j, ], 0, 0, 1, 1, 0.9, burn = 0) # Do not remove.
  phix = rbind(phix, temp[, 1])
  phiy = rbind(phiy, temp[, 2])
}

for(n in 2:m)
{
  GR1 = GelmanRubin(as.matrix(phix)[, 1:n])
  GR2 = GelmanRubin(as.matrix(phiy)[, 1:n])
  if(GR1 < 1.2 & GR2 < 1.2)
  {
    break
  }
}
```

The converge step:

```{r}
n
```

Which is far away from burn-in $= 1000$.

```{r}
rm(x7)
rm(phix)
rm(phiy)
```



# HW8-2022-11-05

## Question

Classwork1.

## Answer

```{r}
Sample = function(N = 20, al, be, gam, intercept = c(0, 0)) # data generator
{
  eps = rnorm(2*N, 0, 1)
  X = runif(N)
  M = intercept[1] + al * X + eps[1:N]
  Y = intercept[2] + be * M + gam * X + eps[-(1:N)]
  return(data.frame(cbind(X, M, Y)))
}
```

```{r}
T.stat = function(dat)
{
  colnames(dat) = c("X", "M", "Y")
  fit1 = lm(M ~ X, data = dat)
  fit2 = lm(Y ~ M + X, data = dat)
  T = fit1$coef[2] * fit2$coef[2] / sqrt(fit1$coef[2] ^ 2 *  (summary(fit1)$coef[2,2]) ^ 2 + fit2$coef[2] ^ 2 * (summary(fit2)$coef[2,2]) ^ 2)
  return(T)
  }
```

```{r}
testalpha = function(dat, R, con)
{
  N = nrow(dat)

  T0 = T.stat(dat)

  Tper = numeric(R)
  for(i in 1:R)
  {
    ix = sample(1:N)
    Tper[i] = T.stat(data.frame(cbind(dat$X[ix], dat$M, dat$Y)))
  }

  p = mean(abs(Tper) >= abs(T0))
  return(p < con)
}
```

```{r}
testbeta = function(dat, R, con)
{
  N = nrow(dat)

  T0 = T.stat(dat)

  Tper = numeric(R)
  for(i in 1:R)
  {
    ix = sample(1:N)
    Tper[i] = T.stat(data.frame(cbind(dat$X, dat$M, dat$Y[ix])))
  }

  p = mean(abs(Tper) >= abs(T0))
  return(p < con)
}
```

```{r}
testalphabeta = function(dat, R, con)
{
  N = nrow(dat)

  T0 = T.stat(dat)

  Tper = numeric(R)
  for(i in 1:R)
  {
    ix = sample(1:N)
    Tper[i] = T.stat(data.frame(cbind(dat$X, dat$M[ix], dat$Y)))
  }

  p = mean(abs(Tper) >= abs(T0))
  return(p < con)
}
```

```{r}
TypeIError = function(al, be, gam, R = 59, N = 100, con = 0.05)
{
  I.error1 = numeric(N)
  I.error2 = numeric(N)
  I.error3 = numeric(N)
  for(i in 1:N)
  {
    dat = Sample(al = al, be = be, gam = gam)
    I.error1[i] = testalpha(dat, R = R, con = con)
    I.error2[i] = testbeta(dat, R = R, con = con)
    I.error3[i] = testalphabeta(dat, R = R, con = con)
  }
  return(c(mean(I.error1), mean(I.error2), mean(I.error3)))
}
```

This sample comes from $\alpha = \beta = 0, \gamma = 1$.

```{r}
set.seed(114514)
TypeIError(0, 0, 1)
```

This sample comes from $\alpha = 0, \beta = \gamma= 1$.

```{r}
set.seed(114514)
TypeIError(0, 1, 1)
```

This sample comes from $\beta = 0, \alpha = \gamma = 1$.

```{r}
set.seed(114514)
TypeIError(1, 0, 1)
```

We can see when $\alpha = 0, \beta = \gamma= 1$, the I-type-error tes of $H_0:\beta = 0$ and $H_0:\alpha = 0 or \beta = 0$ are lost of control.

## Question

Classwork2.

## Answer

(1)

```{r}
Expit = function(N, b1, b2, b3, f0)
{
  X1 = rpois(N, 1)
  X2 = rexp(N, 1)
  X3 = sample(c(0, 1), N, replace = TRUE)
  m = length(f0)
  alpha = numeric(m)
  for(i in 1:m)
  {
    g = function(alpha)
    {
      p = 1 / (1 + exp(- (alpha + b1 * X1 + b2 * X2 + b3 * X3)))
      return(mean(p) - f0[i])
    }
    alpha[i] = uniroot(g, c(-20, 0))$root
  }
  return(alpha)
}
```

(2)

```{r}
set.seed(114514)
alpha = Expit(10^6, 0, 1, -1, c(10^-1, 10^-2, 10^-3, 10^-4))
print(alpha)
```

(3)

```{r}
plot(x = alpha, y = c(10^-1, 10^-2, 10^-3, 10^-4), pch = 20, ylab = "f0")
```

```{r}
rm(alpha)
```



# HW9-2022-11-11

## Question

Classwork(2).

## Answer

#### (2)

We use the formula from the proof above.

MLE method:

```{r}
MLE = function(u, v)
{
  loglh = function(lambda)
  {
    return(-sum(u) + sum((v - u) / (exp(lambda * (v - u)) - 1)))
  }
  return(uniroot(loglh, c(0, max(v))))
}
```

EM method:

```{r}
EM = function(u, v, lambda0 = 1)
{
  iter = 0
  eps = 10 ^ -6
  lambda.next = numeric(1)
  lambda = lambda0
  while(1)
  {
    iter = iter + 1
    lambda.next = 1 / (1 / lambda + mean(u) - mean((v - u) / (exp(lambda * (v - u)) - 1)))
    if(abs(lambda.next - lambda) < eps)
    {
      break
    }
    lambda =lambda.next
  }
  return(c(lambda.hat = lambda.next, iteration = iter))
}
```

```{r}
u = c(11, 8, 27, 13, 16, 0, 23, 10, 24, 2)
v = c(12, 9, 28, 14, 17, 1, 24, 11, 25, 3)
```

```{r}
MLE(u, v)
```

```{r}
EM(u, v)
```

We can see the two method give the same estimate,

```{r}
rm(u)
rm(v)
```

## Question

(2.1.3, Ex 4, 5, Advanced in R)

## Answer

#### 4.

Elements in a vector should be the same type, while a list doen not demand that.So the as.vector() converts the elements into same type and unlist() does not do that.

#### 5.

In 1 == "1", the integer 1 is coerced to the character atomic vector "1", so the result is true.

In -1 < FALSE , the logical atomic vector FALSE is coerced to the integer 0, so the result is true.

In "one" < 2 , the character atomic vector "one" cannot be coerced to the integer 1, so it is false.

## Question

(2.3.1, Ex 1, 2, Advanced in R)

## Answer

#### 1.

NULL. dim() is a function for matrix, data frame and array.

#### 2.

TRUE. A matrix is a kind of array.

## Question

(2.4.5, Ex 1, 2, 3, Advanced in R)

## Answer

#### 1.

Names, row names, and class.

#### 2.

The elements in the data frame will be coerced into the same type and follow the rule that: logical < integer < double < character.

#### 3.

A data frame with 0 row is permitted, but the one with 0 columns is not permitted.



# HW10-2022-11

## Question

(Ex11.1.2.2, Advanced R)

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```

## Answer

We use sapply() to apply the function to every column of a data frame.

```{r}
dat11 = data.frame(matrix(c(1, 2, 3, 4, 5.5, 7), nrow = 3, ncol = 2))
colnames(dat11) = c("X1", "X2")
dat11
sapply(dat11, scale01)
```

If we need to select the numeric column, we can do it with the help of dplyr::select_if().

```{r}
dat12 = cbind(c("a", "b", "c"), dat11, c("x", "y", "z"))
colnames(dat12) = c("X1", "X2", "X3", "X4")
dat12
sapply(dplyr::select_if(dat12, is.numeric), scale01)
```

## Question

(Ex11.2.5.1, Advanced R)

## Answer

(We use data frames the same as the ones in the previous question.)

a)

```{r}
vapply(dat11, sd, numeric(1))
```

b)

```{r}
vapply(dat12[ ,which(vapply(dat12, is.numeric, logical(1)) == TRUE)], sd, numeric(1))
```

```{r}
rm(dat11)
rm(dat12)
```

## Question

Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9.

• Write an Rcpp function.

• Compare the corresponding generated random numbers with pure R language using the function “qqplot”.

• Compare the computation time of the two functions with the function “microbenchmark”.

## Answer

```{r}
library(Rcpp)
cppFunction('NumericMatrix gibbsC(double inix, double iniy, double mu1, double mu2, double sigma1, double sigma2, double rho, int n = 10000, int burn = 1000){

NumericMatrix a(n+burn, 2);
double s1 = sqrt(1 - rho * rho) * sigma1, s2 = sqrt(1 - rho * rho) * sigma2 ;

a(0,0) = inix, a(0,1) = iniy;

for (int i = 1; i <= n+burn-1; i++)
{
  double tempy = a(i-1,1);
  double m1 = mu1 + rho * (tempy - mu2) * sigma1 / sigma2;
  a(i,0) = rnorm(1, m1, s1)[0];

  double tempx = a(i,0);
  double m2 = mu2 + rho * (tempx - mu1) * sigma2/sigma1;
  a(i,1) = rnorm(1, m2, s2)[0];
}

return (a);
}')
```

```{r}
gibbsR = function(ini, mu1, mu2, sigma1, sigma2, rho, n = 10000, burn = 1000)
{
  x = matrix(0, n + burn, 2)
  s1 = sqrt(1 - rho ^ 2) * sigma1
  s2 = sqrt(1 - rho ^ 2) * sigma2

  x[1, ] = ini
  for (i in 2:(n + burn))
  {
    x2 = x[i - 1, 2]
    m1 = mu1 + rho * (x2 - mu2) * sigma1/sigma2
    x[i, 1] = rnorm(1, m1, s1)
    x1 = x[i, 1]
    m2 = mu2 + rho * (x1 - mu1) * sigma2/sigma1
    x[i, 2] = rnorm(1, m2, s2)
  }

  return(x)
}
```

```{r}
set.seed(114514)
C = gibbsC(100, 10, 0, 0, 1, 1, 0.9)[-(1:1000), ]
R = gibbsR(c(100, 10), 0, 0, 1, 1, 0.9)[-(1:1000), ]
```

```{r}
plot(sort(C[,1]), sort(R[,1]), main="qqplot of x1", xlab="XC", ylab="XR")
plot(sort(C[,2]), sort(R[,2]), main="qqplot of x2", xlab="XC", ylab="XR")
```

The two method gain similar result.

```{r}
ts = microbenchmark::microbenchmark(gibbC = gibbsC(100, 10, 0, 0, 1, 1, 0.9), gibbR = gibbsR(c(100, 10), 0, 0, 1, 1, 0.9))
summary(ts)[, c(1, 3, 5, 6)]
```

We see that much less time the Cpp method costs than the pure R code costs.

```{r}
rm(C)
rm(R)
```
